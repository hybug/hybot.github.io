---
layout: post 
title: 关于正则化那些事
subtitle: 算法 
date: 2019-3-4 
author: Hybot 
header-img: img/post-bg-shiyuan1.jpg 
catalog: true 
tags: tensorflow
---

## 1. 正则化公式

> 正则化：是一种常用的为了避免过度拟合而采用的一种算法。正则化的主要思想是通过在损失函数中加入刻画模型复杂程度的指标，
假设模型的损失函数为J(θ)，那么我们在使用优化算法来优化损失函数的时候，不是直接优化J(θ)，而是优化J(θ)+ λ *R(w)。
其中R(w)是指模型的复杂程度，λ表示模型复杂损失在总损失中的比例。需要注意的是这里的θ表示的是一个神经网络中的所有参数，
它包括权重和偏置。一般来说，模型复杂度只由其权重（w）来决定。

- L1正则化

![](https://github.com/hybug/hybug.github.io/blob/master/img/L1_reg.png?raw=true)

- L2正则化

![](https://github.com/hybug/hybug.github.io/blob/master/img/L2_reg.png?raw=true)

> 无论是L1正则化还是L2正则化，两者的目的都是通过限制权重的大小，来使得模型不能任意的拟合训练数据中的随机噪音。
两者正则化也有很大的区别。L1正则化会让参数变得稀疏，而L2正则化则不会产生这个问题。参数稀疏是指会有更多的参数变为0，
其实就相当于达到了类似于特征选取的功能。L2正则化不会产生稀疏的原因在于，当参数很小的时候，参数的平方基本上就趋近于0，
可以忽略了，而模型不会进一步将这个参数调整为0。L1正则化的计算公式不可导，L2正则化可导。而在优化模型的时候需要计算损失函数的偏导数，
所以对于这种情况L2比较合适。

## 2. 正则化实现

### 2.1 regularization in tensorflow

将正则化加入了所有可以训练的weights参数上

```
cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels)
policy_loss = tf.reduce_mean(cross_entropy)
# using l2 regularization
l2_reg = tf.contrib.layers.l2_regularizer(scale=0.001)
reg_term = tf.contrib.layers.apply_regularization(l2_reg, tf.trainable_variables())
loss = policy_loss + reg_term
```

### 2.2 regularization in keras

- kernel_regularizer：施加在权重上的正则项，为keras.regularizer.Regularizer对象

- bias_regularizer：施加在偏置向量上的正则项，为keras.regularizer.Regularizer对象

- activity_regularizer：施加在输出上的正则项，为keras.regularizer.Regularizer对象

```
# A linear layer with L1 regularization of factor 0.01 applied to the kernel matrix:
layers.Dense(64, kernel_regularizer=tf.keras.regularizers.l1(0.01))
# add reg in keras
model.add(Dense(64, input_dim=64,
                kernel_regularizer=regularizers.l2(0.01),
                activity_regularizer=regularizers.l1(0.01)))
```


