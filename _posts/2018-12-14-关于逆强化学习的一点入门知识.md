---
layout:     post
title:      关于逆强化学习的一点入门知识
subtitle:   算法详解
date:       2018-12-14
author:     Hybot
header-img: img/post-md.jpg
catalog: true
tags:
    - AI
    - DRL
    - IDRL
---

有些东西当你深入研究了之后，才明白某个技术在各个方面的细节。任何一篇DRL入门博客都会告诉你，DRL其中一个缺点就是稀疏奖励（sparse reward）的问题，当时没接触到项目，也没关注这个事情，后来深入研究后发现这个问题真是DRL的死穴之一。Google了很多关于如何解决sparse reward的方法，后面会专门写一篇博客在介绍，这儿大概介绍一下在查阅过程中发现的一个新的知识点，就是逆强化学习，就当做个todo list之后如果有用到，再回来补充技术细节。

因为也没有深入研究，与其说是博客，不如说是笔记吧，参考资料来源：[Apprenticeship learning using Inverse Reinforcement Learning](https://jangirrishabh.github.io/2016/07/09/virtual-car-IRL/)

# 1.逆强化学习分类

![](https://raw.githubusercontent.com/hybug/hybug.github.io/master/img/20181214-pic%20(1).png)

## 1.1 逆强化学习可以解决什么问题？

奖励稀疏的问题解决方法之一就是：**设置合适的奖励函数**，在简单的DRL任务中，我们都假设奖励函数是简单的+1，-1对，到达了goal给予+1，否则就给予-1。然而任务复杂一点，比如要训练一个踢足球的agent，这样的奖励函数带来的便是极度稀疏的奖励回馈，神经网络得到的永远是-1，-1，自然也学不到任何东西。

面对这样的问题，我们当然可以人为的设计巧妙合适的奖励函数：比如下面的奖励函数就可能train出我们想要的结果：

**Reward Function = M1&times;是否进球 + M2&times;脚与足球的距离 + M3&times;足球的速度 + ...**

但是这样的奖励函数有很强的经验性和主观性，人为设置奖励函数的有如下缺点：

- 面对复杂任务，奖励函数本来无法直观的给出。比如机械臂的抓取问题
- 当任务不同的时候，奖励函数也不同，难道我们需要一个专门给agent设置奖励函数的技术人员吗？那我们的agent就不是人工智能而是人工智障了，这个问题就是强化学习算法应用与日常的最大问题

逆强化学习就是来解决上述问题而存在的。

## 1.2 逆强化学习如何学习奖励函数的设置？

RL一直在试图让agent像人一个行动，无论是游戏也好，机械臂操作也好，都是试验和错误学习最直观的形式，这也是大多数有思维能力的生物体的学习方式，这是新出生的人类婴儿最开始的学习方式，就是通过最初的随机动作然后慢慢探索学习爬行的动作。

但是，人类真的是这样学习的吗？每个人都是在手被火焰烫过之后才知道火焰不能用手去触摸的吗？答案当然是NO，在我们触摸火焰之前，我们的父母就会告诉我们，这个摸了会被烫，如果你足够听话，那你就可以避免这一次“试错行动”而学习到“火焰不可以触摸”这个知识，当然这样的代价是我们可能会丢失探索世界的好奇心(curiosity)，（这个问题我们之后再解决）

发现了吗？原来比随机探索更有效的是模仿父母（专家）的学习模式，其实逆强化学习就是原来于模仿学习，人在学习很多技能的时候都是从模仿开始的。但有人只模仿到了表面，而有人模仿到了精髓。最早的模仿学习是行为克隆，它只模仿到了表面。在行为克隆中，人的示例轨迹被记录下来，下次执行时恢复该轨迹。行为克隆的方法只能模仿轨迹，无法进行泛化。而逆向强化学习是从专家（人为）示例中学到背后的回报函数，能泛化到其他情况，因此属于模仿到了精髓。

本文讲解的就是吴恩达和Abbeel提出的学徒学习（Apprenticeship Learning）,注意该算法在2000年就提出了 [Algorithms for Inverse Reinforcement Learning](http://ai.stanford.edu/~ang/papers/icml00-irl.pdf)

### 1.2.1 学徒学习原理

本文只会学徒学习的基本原理进行简单说明，不涉及公式推导（我也不会）。

学徒学习任务：智能体从专家示例中学到回报函数，使得在该回报函数下所得到的最优策略在专家示例策略附近。

> 未知的回报函数![R(s)]一般都是状态的函数，因为它是未知的，所以我们可以利用函数逼近的方法对其进行参数逼近，其逼近形式可设为：![R\left(s\right)=w\cdot\phi\left(s\right)](http://www.zhihu.com/equation?tex=R%5Cleft%28s%5Cright%29%3Dw%5Ccdot%5Cphi%5Cleft%28s%5Cright%29)，其中![\phi(s)](http://www.zhihu.com/equation?tex=%5Cphi%28s%29)为基函数，可以为多项式基底，也可以为傅里叶基底。逆向强化学习求的是回报函数中的系数![w](http://www.zhihu.com/equation?tex=w)。

我们要做的就是：找到一个策略让其表示和专家策略差不多，在数学上表示就是两者的特征期望相近。

### 1.2.2 学徒学习算法

学徒逆向强化学习算法分为两步，第一步在已经迭代得到的最优策略中，利用最大边际方法求出当前的回报函数的参数值；第二步利用求出的回报函数的参数值进行正向强化学习方法求得当前最优的策略，然后重复第一步。

**重要定义**

- 基函数：就是奖励函数的基函数 ![\psi](http://www.zhihu.com/equation?tex=%5Cphi%28s%29)


- 奖励函数：![R\left(s\right)=w\cdot\phi\left(s\right)](http://www.zhihu.com/equation?tex=R%5Cleft%28s%5Cright%29%3Dw%5Ccdot%5Cphi%5Cleft%28s%5Cright%29)
- 特征期望：![](https://raw.githubusercontent.com/hybug/hybug.github.io/master/img/20181214-pic%20(2).png)

**算法**

1. 生成一个list存放每次迭代后获得的策略特征期望

2. 在迭代初始我们只有π1-->随机策略 的特征期望

3. 使用最大边界规划求得第一个![w](http://www.zhihu.com/equation?tex=w)1，这个过程和SVM分类问题类似。我们专家策略特征期望赋值为正，其他的策略特征期望为负，最小化两者之和，终止条件为：![](https://raw.githubusercontent.com/hybug/hybug.github.io/master/img/20181214-pic%20(3).png)

4. 现在，一旦我们在一次优化迭代后获得权重，即一旦我们获得新的奖励函数，我们就必须学习这种奖励函数产生的策略。 这就像说，找到一个试图最大化这个获得的奖励功能的政策。 为了找到这个新策略，我们必须使用这个新的奖励函数训练强化学习算法，并训练它直到Q值收敛，以获得对策略的适当估计。

5. 在我们学习了新政策后，我们必须在线测试此政策，以获得与此新政策相对应的特征预期。 然后我们将这些新特征期望值添加到我们的特征期望列表中，并继续进行IRL算法的下一次迭代直到收敛。

### 1.2.3 IRL代码实现

jangirrishabh在[toyCarIRL-github](https://github.com/jangirrishabh/toyCarIRL)实现了IRL。

### 1.2.3 参考资料
[https://jangirrishabh.github.io/2016/07/09/virtual-car-IRL/](https://jangirrishabh.github.io/2016/07/09/virtual-car-IRL/)
