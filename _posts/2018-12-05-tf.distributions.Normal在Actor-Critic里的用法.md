---
layout:     post
title:      tf.distributions.Normal在Actor-Critic里的用法
subtitle:   算法解析
date:       2018-12-05
author:     Hybot
header-img: img/post-md.jpg
catalog: true
tags:
    - AI
    - DRL
    - Actor-Critic
    - tensorflow
---

最近在看Actor-Critic算法，这个算法是Value-based算法Q-Learning和Policy-based算法Policy gradients算法的结合，他可以单步更新连续的action值，具体的算法这里不做仔细说明。需要明白的是，和DQL不同，我们的网络输出的是连续的值，而不是离散的，那么怎么做到呢？

Actor其实是一个Policy Network ,那么他就需要奖惩信息来进行调节不同状态下采取各种动作的概率，在传统的Policy Gradient算法中，这种奖惩信息是通过走完一个完整的episode来计算得到的。这不免导致了学习速率很慢，需要很长时间才可以学到东西。既然Critic是一个以值为基础的学习法，那么他可以进行单步更新，计算每一步的奖惩值。那么二者相结合，Actor来选择动作，Critic来告诉Actor它选择的动作是否合适。在这一过程中，Actor不断迭代，得到每一个状态下选择每一动作的合理概率，Critic也不断迭代，不断完善每个状态下选择每一个动作的奖惩值。

Actor网络代码如下参考：

```
class Actor(object):
    def __init__(self, sess, n_features, action_bound, lr=0.0001):
        self.sess = sess

        self.s = tf.placeholder(tf.float32, [1, n_features], "state")
        self.a = tf.placeholder(tf.float32, None, name="act")
        self.td_error = tf.placeholder(tf.float32, None, name="td_error")  # TD_error

        l1 = tf.layers.dense(
            inputs=self.s,
            units=30,  # number of hidden units
            activation=tf.nn.relu,
            kernel_initializer=tf.random_normal_initializer(0., .1),  # weights
            bias_initializer=tf.constant_initializer(0.1),  # biases
            name='l1'
        )

        mu = tf.layers.dense(
            inputs=l1,
            units=1,  # number of hidden units
            activation=tf.nn.tanh,
            kernel_initializer=tf.random_normal_initializer(0., .1),  # weights
            bias_initializer=tf.constant_initializer(0.1),  # biases
            name='mu'
        )

        sigma = tf.layers.dense(
            inputs=l1,
            units=1,  # output units
            activation=tf.nn.softplus,  # get action probabilities
            kernel_initializer=tf.random_normal_initializer(0., .1),  # weights
            bias_initializer=tf.constant_initializer(1.),  # biases
            name='sigma'
        )
        global_step = tf.Variable(0, trainable=False)
        # self.e = epsilon = tf.train.exponential_decay(2., global_step, 1000, 0.9)
        ##squeeze 去掉值为1的维度[[0]] ----> 0
        self.mu, self.sigma = tf.squeeze(mu*2), tf.squeeze(sigma+0.1)

        self.normal_dist = tf.distributions.Normal(self.mu, self.sigma)
        ##tf.clip_by_value:输入一个张量A，把A中的每一个元素的值都压缩在min和max之间。小于min的让它等于min，大于max的元素的值等于max。
        self.action = tf.clip_by_value(self.normal_dist.sample(1), action_bound[0], action_bound[1])

        with tf.name_scope('exp_v'):
            log_prob = self.normal_dist.log_prob(self.a)  # loss without advantage
            self.exp_v = log_prob * self.td_error  # advantage (TD_error) guided loss
            # Add cross entropy cost to encourage exploration
            self.exp_v += 0.01*self.normal_dist.entropy()

        with tf.name_scope('train'):
            self.train_op = tf.train.AdamOptimizer(lr).minimize(-self.exp_v, global_step)    # min(v) = max(-v)

    def learn(self, s, a, td):
        s = s[np.newaxis, :]
        feed_dict = {self.s: s, self.a: a, self.td_error: td}
        _, exp_v = self.sess.run([self.train_op, self.exp_v], feed_dict)
        return exp_v

    def choose_action(self, s):
        s = s[np.newaxis, :]
        return self.sess.run(self.action, {self.s: s})  # get probabilities for all actions
```

我们只关注输入输出，输入状态s，网络得到mu和sigma。

tf.distributions.normal可以生成一个均值为loc，方差为scale的正态分布。

tf.clip_by_value将正态分布输出值压缩在min~max之间得到action输出。下面说明一下tf.distributions.normal用到的方法。

1. sample()

![](https://github.com/hybug/hybug.github.io/blob/master/img/20181205-pic7?raw=true)

2. log_prob()

输出value值，在正态分布中概率的log值，作为loss值
